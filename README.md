My implementation uses Bernstein’s hash function combined with a custom secondary hash function to apply double hashing for managing the dictionary. The hash table is initialized with a prime size (98317), and a remainder-based distribution is used to minimize collisions. To reduce the risk of collisions, dynamic resizing is employed with a load factor threshold (0.75), a standard value that balances memory usage and performance. When the load factor exceeds this threshold, the hash table is dynamically resized by doubling its capacity. This resizing helps to maintain performance as the dictionary grows, though it also increases memory consumption. The approach is simpler to implement than a trie-based structure, which requires complex traversal, particularly for incomplete words not added in dict_addword, and has a time complexity of O(N), while hashing achieves O(1) average time complexity for insertions, deletions, and lookups. 

Less common letters like 'v', 'w', and 'x' clustering fewer nodes, and more frequent letters like 't', 'r', and 'a' leading to suboptimal search times for certain words while using tries. Furthermore, while Bernstein’s hash function is simple it is not optimal for larger datasets, leading to higher collision rates as the dictionary grew and therefore the performance of the hash table degraded, particularly with sequential search for collision resolution. Although double hashing reduced this issue, the success of the method was heavily dependent on selecting an effective secondary hash function. I experimented with different approaches and eventually settled on a secondary function i found on StackOverflow using modulo 17, adjusted by the table’s capacity, which improved key distribution. 

In comparison to tries, the hashing approach was more straightforward to implement and offered better average-case performance. However, the main trade-off with hashing is the time-space tradeoff: while tries may be faster for small datasets, hash tables require resizing and memory adjustments as they grow, potentially leading to higher memory consumption. Additionally, more advanced hash functions, such as Jenkins or MurmurHash, could further improve key distribution and performance by reducing collisions and providing a more even spread of keys. 
